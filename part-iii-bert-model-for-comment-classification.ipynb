{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toxic Comment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"ticks\", context=\"talk\")\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import transformers\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import tokenizers\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score, roc_curve, auc\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "train = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip', nrows = 2000)\n",
    "test = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv.zip', nrows = 100)\n",
    "submission = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip')\n",
    "\n",
    "SEED = 34\n",
    "def random_seed(SEED):\n",
    "    random.seed(SEED)\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "random_seed(SEED)\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "train['clean_text'] = train['comment_text'].apply(str).apply(lambda x: clean_text(x))\n",
    "test['clean_text'] = test['comment_text'].apply(str).apply(lambda x: clean_text(x))\n",
    "\n",
    "kfold = 5\n",
    "train['kfold'] = train.index % kfold\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-cased')\n",
    "max_len = 200\n",
    "\n",
    "class BertDataSet(Dataset):\n",
    "    \n",
    "    def __init__(self, sentences, toxic_labels):\n",
    "        self.sentences = sentences\n",
    "        #target is a matrix with shape [#1 x #6(toxic, obscene, etc)]\n",
    "        self.targets = toxic_labels.to_numpy()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        bert_senten = tokenizer.encode_plus(sentence, \n",
    "                                            add_special_tokens = True, # [CLS],[SEP]\n",
    "                                            max_length = max_len,\n",
    "                                            pad_to_max_length = True,\n",
    "                                            truncation = True,\n",
    "                                            return_attention_mask = True\n",
    "                                             )\n",
    "        ids = torch.tensor(bert_senten['input_ids'], dtype = torch.long)\n",
    "        mask = torch.tensor(bert_senten['attention_mask'], dtype = torch.long)\n",
    "        toxic_label = torch.tensor(self.targets[idx], dtype = torch.float)\n",
    "        \n",
    "        \n",
    "        return {\n",
    "            'ids' : ids,\n",
    "            'mask' : mask,\n",
    "            'toxic_label':toxic_label\n",
    "        }\n",
    "\n",
    "epochs = 5\n",
    "train_batch = 32\n",
    "valid_batch = 32\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "loss_fn.to(device)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "def training(train_dataloader, model, optimizer, scheduler):\n",
    "    model.train()\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for a in train_dataloader:\n",
    "        losses = []\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #allpreds = []\n",
    "        #alltargets = []\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            \n",
    "            ids = a['ids'].to(device, non_blocking = True)\n",
    "            mask = a['mask'].to(device, non_blocking = True) \n",
    "\n",
    "            output = model(ids, mask) #This gives model as output, however we want the values at the output\n",
    "            output = output['logits'].squeeze(-1).to(torch.float32)\n",
    "\n",
    "            output_probs = torch.sigmoid(output)\n",
    "            preds = torch.where(output_probs > 0.5, 1, 0)\n",
    "            \n",
    "            toxic_label = a['toxic_label'].to(device, non_blocking = True) \n",
    "            loss = loss_fn(output, toxic_label)            \n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            #allpreds.append(output.detach().cpu().numpy())\n",
    "            #alltargets.append(toxic.detach().squeeze(-1).cpu().numpy())\n",
    "            correct_predictions += torch.sum(preds == toxic_label)\n",
    "        \n",
    "        scaler.scale(loss).backward() #Multiplies (‘scales’) a tensor or list of tensors by the scale factor.\n",
    "                                      #Returns scaled outputs. If this instance of GradScaler is not enabled, outputs are returned unmodified.\n",
    "        scaler.step(optimizer) #Returns the return value of optimizer.step(*args, **kwargs).\n",
    "        scaler.update() #Updates the scale factor.If any optimizer steps were skipped the scale is multiplied by backoff_factor to reduce it. \n",
    "                        #If growth_interval unskipped iterations occurred consecutively, the scale is multiplied by growth_factor to increase it\n",
    "        scheduler.step() # Update learning rate schedule\n",
    "    \n",
    "    losses = np.mean(losses)\n",
    "    corr_preds = correct_predictions.detach().cpu().numpy()\n",
    "    accuracy = corr_preds/(len(p_train)*6)\n",
    "    \n",
    "    return losses, accuracy\n",
    "\n",
    "def validating(valid_dataloader, model):\n",
    "    \n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    all_output_probs = []\n",
    "    \n",
    "    for a in valid_dataloader:\n",
    "        losses = []\n",
    "        ids = a['ids'].to(device, non_blocking = True)\n",
    "        mask = a['mask'].to(device, non_blocking = True)\n",
    "        output = model(ids, mask)\n",
    "        output = output['logits'].squeeze(-1).to(torch.float32)\n",
    "        output_probs = torch.sigmoid(output)\n",
    "        preds = torch.where(output_probs > 0.5, 1, 0)\n",
    "            \n",
    "        toxic_label = a['toxic_label'].to(device, non_blocking = True)\n",
    "        loss = loss_fn(output, toxic_label)\n",
    "        losses.append(loss.item())\n",
    "        all_output_probs.extend(output_probs.detach().cpu().numpy())\n",
    "        \n",
    "        correct_predictions += torch.sum(preds == toxic_label)\n",
    "        corr_preds = correct_predictions.detach().cpu().numpy()\n",
    "    \n",
    "    losses = np.mean(losses)\n",
    "    corr_preds = correct_predictions.detach().cpu().numpy()\n",
    "    accuracy = corr_preds/(len(p_valid)*6)\n",
    "    \n",
    "    return losses, accuracy, all_output_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "best_scores = []\n",
    "for fold in tqdm(range(0,5)):\n",
    "\n",
    "    # initializing the data\n",
    "    p_train = train[train['kfold'] != fold].reset_index(drop = True)\n",
    "    p_valid = train[train['kfold'] == fold].reset_index(drop = True)\n",
    "\n",
    "    train_dataset = BertDataSet(p_train['clean_text'], p_train[['toxic', 'severe_toxic','obscene', 'threat', 'insult','identity_hate']])\n",
    "    valid_dataset = BertDataSet(p_valid['clean_text'], p_valid[['toxic', 'severe_toxic','obscene', 'threat', 'insult','identity_hate']])\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size = train_batch, shuffle = True, num_workers = 4, pin_memory = True)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size = valid_batch, shuffle = False, num_workers = 4, pin_memory = True)\n",
    "\n",
    "    model = transformers.BertForSequenceClassification.from_pretrained(\"../input/bert-base-cased\", num_labels = 6)\n",
    "    model.to(device)\n",
    "    \n",
    "    LR = 2e-5\n",
    "    optimizer = AdamW(model.parameters(), LR,betas = (0.9, 0.999), weight_decay = 1e-2) # AdamW optimizer\n",
    "\n",
    "    train_steps = int(len(p_train)/train_batch * epochs)\n",
    "    num_steps = int(train_steps * 0.1)\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_steps, train_steps)\n",
    "    \n",
    "    best_score = 1000\n",
    "    train_accs = []\n",
    "    valid_accs = []\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    best_valid_probs = []\n",
    "    \n",
    "    print(\"-------------- Fold = \" + str(fold) + \"-------------\")\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        print(\"-------------- Epoch = \" + str(epoch) + \"-------------\")\n",
    "\n",
    "        train_loss, train_acc = training(train_dataloader, model, optimizer, scheduler)\n",
    "        valid_loss, valid_acc, valid_probs = validating(valid_dataloader, model)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_accs.append(valid_acc)\n",
    "        \n",
    "        print('train losses: %.4f' %(train_loss), 'train accuracy: %.3f' %(train_acc))\n",
    "        print('valid losses: %.4f' %(valid_loss), 'valid accuracy: %.3f' %(valid_acc))\n",
    "\n",
    "        if (valid_loss < best_score):\n",
    "\n",
    "            best_score = valid_loss\n",
    "            print(\"Found an improved model! :)\")\n",
    "\n",
    "            state = {'state_dict': model.state_dict(),\n",
    "                     'optimizer_dict': optimizer.state_dict(),\n",
    "                     'best_score':best_score\n",
    "                    }\n",
    "\n",
    "            torch.save(state, \"model\" + str(fold) + \".pth\")\n",
    "            best_valid_prob = valid_probs\n",
    "            torch.cuda.memory_summary(device = None, abbreviated = False)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "    best_scores.append(best_score)\n",
    "    best_valid_probs.append(best_valid_prob)\n",
    "    \n",
    "    ##Plotting the result for each fold\n",
    "    x = np.arange(epochs)\n",
    "    fig, ax = plt.subplots(1, 2, figsize = (15,4))\n",
    "    ax[0].plot(x, train_losses)\n",
    "    ax[0].plot(x, valid_losses)\n",
    "    ax[0].set_ylabel('Losses', weight = 'bold')\n",
    "    ax[0].set_xlabel('Epochs')\n",
    "    ax[0].grid(alpha = 0.3)\n",
    "    ax[0].legend(labels = ['train losses', 'valid losses'])\n",
    "\n",
    "    ax[1].plot(x, train_accs)\n",
    "    ax[1].plot(x, valid_accs)\n",
    "    ax[1].set_ylabel('Accuracy', weight = 'bold')\n",
    "    ax[1].set_xlabel('Epochs')\n",
    "    ax[1].legend(labels = ['train acc', 'valid acc'])\n",
    "\n",
    "    ax[1].grid(alpha = 0.3)\n",
    "    fig.suptitle('Fold = '+str(fold), weight = 'bold') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean of',kfold, 'folds for best loss in', epochs, 'epochs cross-validation folds is %.4f.' %(np.mean(best_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicting(test_dataloader, model, pthes):\n",
    "    allpreds = []\n",
    "    \n",
    "    for pth in pthes:\n",
    "        state = torch.load(pth)\n",
    "        model.load_state_dict(state['state_dict'])\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for a in test_dataloader:\n",
    "                ids = a['ids'].to(device)\n",
    "                mask = a['mask'].to(device)\n",
    "                output = model(ids, mask)\n",
    "                output = output['logits'].squeeze(-1)\n",
    "                output_probs = torch.sigmoid(output)\n",
    "                preds.append(output_probs.cpu().numpy())\n",
    "            preds = np.concatenate(preds)\n",
    "            allpreds.append(preds)\n",
    "      \n",
    "    return allpreds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pthes = [os.path.join(\"./\",s) for s in os.listdir(\"./\") if \".pth\" in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allpreds = predicting(valid_dataloader, model, pthes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_probs = np.zeros((len(p_valid),6))\n",
    "for i in range(kfold):\n",
    "    valid_probs += allpreds[i]\n",
    "valid_probs = valid_probs / kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_probs = np.asarray(valid_probs).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid_probs = allpreds[0].flatten() #This line is used when trianing for one model and not k-fold model \n",
    "y_valid = p_valid[['toxic', 'severe_toxic','obscene', 'threat', 'insult','identity_hate']].to_numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y_valid, valid_probs)\n",
    "print('auc score for kfold =', kfold, 'models is: %.2f' %(auc(fpr, tpr)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(fpr, tpr)\n",
    "ax.set_title('ROC Curv')\n",
    "ax.set_xlabel('FPR')\n",
    "ax.set_ylabel('TPR')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTinferenceDataSet(Dataset):\n",
    "    \n",
    "    def __init__(self, sentences):\n",
    "        self.sentences = sentences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        bert_sent = tokenizer.encode_plus(sentence, \n",
    "                                         add_special_tokens = True, #[SEP][PAD]\n",
    "                                         max_length = max_len,\n",
    "                                         pad_to_max_length = True,\n",
    "                                         truncation = True)\n",
    "\n",
    "        ids = torch.tensor(bert_sent['input_ids'], dtype = torch.long)\n",
    "        mask = torch.tensor(bert_sent['attention_mask'], dtype = torch.long)\n",
    "\n",
    "        return{\n",
    "            'ids' : ids,\n",
    "            'mask' : mask\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data = pd.read_csv('/kaggle/input/tweetdata/biden_tweets_clean.csv')\n",
    "tweets_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data['clean_text'] = tweets_data['comment_text'].apply(str).apply(lambda x: clean_text(x))\n",
    "\n",
    "tweets_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #reducing data set from 1167 to 16\n",
    "# tweets_data = tweets_data.iloc[:-2264]\n",
    "len(tweets_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = 32\n",
    "test_dataset = BERTinferenceDataSet(tweets_data['clean_text'])\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = test_batch, shuffle = False, num_workers = 4, pin_memory = True)\n",
    "pthes = [os.path.join(\"../input/final-models\",s) for s in os.listdir('../input/final-models') if \".pth\" in s]\n",
    "pthes\n",
    "#/kaggle/input/final-models\n",
    "model = transformers.BertForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allpreds = predicting(test_dataloader, model, pthes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('allpreds is an array with the shape of:',len(allpreds), 'x',len(allpreds[0]), 'x',len(allpreds[0][0]))\n",
    "allpreds[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.zeros((len(test_dataset),6))\n",
    "for i in range(kfold):\n",
    "    preds += allpreds[i]\n",
    "preds = preds / kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(preds)\n",
    "submission = pd.concat([test,results], axis = 1).drop(['comment_text', 'clean_text'], axis = 1)\n",
    "submission.rename(columns = { 0:'toxic', 1:'severe_toxic', 2:'obscene', 3:'threat', 4:'insult', 5:'identity_hate'}, inplace = True)\n",
    "submission.to_csv(\"submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.read_csv('/kaggle/working/submission.csv')\n",
    "s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "submission[\"toxicity_category\"] = \"Non-toxic\"\n",
    "\n",
    "# set the threshold\n",
    "threshold = 0.2\n",
    "\n",
    "# create a dictionary to keep the count of each category\n",
    "category_counts = {'toxic':0,'severe_toxic':0,'obscene':0,'threat':0,'insult':0,'identity_hate':0,'Non-toxic':0}\n",
    "\n",
    "#iterate over all the categories \n",
    "for col in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']:\n",
    "    submission.loc[submission[col] > threshold, \"toxicity_category\"] = col\n",
    "    category_counts[col] = submission[submission[\"toxicity_category\"] == col].shape[0]\n",
    "\n",
    "# non-toxic comments\n",
    "category_counts['Non-toxic'] = submission[submission[\"toxicity_category\"] == \"Non-toxic\"].shape[0]\n",
    "\n",
    "# Data to plot\n",
    "labels = list(category_counts.keys())\n",
    "sizes = [v/len(submission)*100 for v in category_counts.values()]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "# Plot\n",
    "plt.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Read the csv file\n",
    "submission = pd.read_csv(\"submission.csv\")\n",
    "\n",
    "# Create a new column \"toxicity_category\" and categorize the comments\n",
    "submission[\"toxicity_category\"] = \"Non-toxic\"\n",
    "\n",
    "# set the threshold\n",
    "threshold = 0.3\n",
    "\n",
    "# create a dictionary to keep the count of each category\n",
    "category_counts = {'toxic':0,'severe_toxic':0,'obscene':0,'threat':0,'insult':0,'identity_hate':0,'Non-toxic':0}\n",
    "\n",
    "#iterate over all the categories \n",
    "for col in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']:\n",
    "    submission.loc[submission[col] > threshold, \"toxicity_category\"] = col\n",
    "    category_counts[col] = submission[submission[\"toxicity_category\"] == col].shape[0]\n",
    "\n",
    "# non-toxic comments\n",
    "category_counts['Non-toxic'] = submission[submission[\"toxicity_category\"] == \"Non-toxic\"].shape[0]\n",
    "\n",
    "# Data for the chart\n",
    "labels = list(category_counts.keys())\n",
    "counts = list(category_counts.values())\n",
    "plt.figure(figsize=(15,8))\n",
    "# Create the bar chart\n",
    "plt.bar(labels, counts)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Toxicity Category')\n",
    "plt.ylabel('Number of Comments')\n",
    "plt.title('Toxicity Categories Distribution')\n",
    "\n",
    "# Show the chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[('@test','What the hell')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('yt_s3.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    header = ['Handle', 'Text']\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "    writer.writerows(data)\n",
    "    \n",
    "yt_s3 = pd.read_csv('/kaggle/working/yt_s3.csv')\n",
    "yt_s3['clean_text'] = yt_s3['Text'].apply(str).apply(lambda x: clean_text(x))\n",
    "\n",
    "yt_s3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = 32\n",
    "test_dataset = BERTinferenceDataSet(yt_s3['clean_text'])\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = test_batch, shuffle = False, num_workers = 4, pin_memory = True)\n",
    "pthes = [os.path.join(\"../input/final-models\",s) for s in os.listdir('../input/final-models') if \".pth\" in s]\n",
    "pthes\n",
    "#/kaggle/input/final-models\n",
    "model = transformers.BertForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allpreds = predicting(test_dataloader, model, pthes)\n",
    "preds = np.zeros((len(test_dataset),6))\n",
    "for i in range(kfold):\n",
    "    preds += allpreds[i]\n",
    "preds = preds / kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(preds)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
